\documentclass[sigplan,review,anonymous]{acmart}

\settopmatter{printfolios=true,printccs=false,printacmref=false}

\input{preamble}
\input{macros}

\begin{document}
\title[Approximating Type Stability in the Julia JIT (work in progress)]{Approximating Type Stability in the Julia JIT\\(work in progress)}
\input{metadata}

\maketitle

\section{Introduction}

The scientific computing community has long been of two minds about the language
technology best suited for the task. On the one hand, the exploratory nature of
programming with raw datasets and mathematical models with many parameters calls
for modern convenience features like dynamic typing and garbage collection. On
the other, crunching numbers may require to squeeze every CPU cycle available.
Accordingly, the language market in this area is split into productivity
languages (Python, MATLAB, R) and high-performance oriented languages (C,
C++, Fortran).

The splitted nature of the language landscape in the area of scientific
computing calls for a question: can we have the best of both worlds? Julia
attempts to answer in the positive.

On the surface, the Juila language has a lot of pythonistic feeling to it.
Consider the following function that sums an array of elements but clamps the
summands above the given threshold.
%
\begin{verbatim}
  function sum(v, t)
    res = v[1]
    for i = 2:length(v)
      elm  = v[i] < t ? v[i] : t
      res = res + elm
    end
    res
  end
\end{verbatim}
%
What strikes about this code is the absence of type annotations either on the
function arguments or anywhere locally. Of course, it is possible to call
\c{sum} with a string and a number and get a runtime failure. On the other hand,
and no less of a surprise, when called with the right arguments the code gets
JIT-compiled and performs on par with equivalent C code.

Julia's competitive performance is documented both in the language manual and in
academic papers~\cite{oopsla18a}. One of the key techniques enabling
essential optimizations is based on the code property called \emph{type
stability}~\cite{Pelenitsyn21}. While well-understood on the IR level, the
property has been elusive for end users: it is mentioned in the manual and on
many forum threads but still does not have a clear source-level model for it.
In this short paper we propose the first algorithm to approximate this highly
dynamic property statically. In particular, we
\begin{itemize}

  \item give an informal description of type stability as it comes up in
  practice of Julia programming (\secref{sec:back}; largely following~\cite{Pelenitsyn21});

  \item discuss challenges to model type stability statically and the relation
  of the task to the full-fledged type inference (\secref{sec:arrive});

  \item give an algorithm to approximate type stability statically (\secref{sec:algo});

  \item remark on implementation (\secref{sec:impl}) and
  evaluation of the algorithm (\secref{sec:eval}) --- both work in progress.
\end{itemize}

% \section{A Typeful Language in The Untyped Universe}%
\section{A Type Stability Primer}%
% \section{Julia: Productivity And Performance Reconciled}
\label{sec:back}

\subsection{Multiple Dispatch in Julia}%

The Julia language is designed around multiple dispatch~\cite{BezansonEKS17}.
Programs consist of \emph{functions} that are implemented by multiple
\emph{methods}; there is nothing more to a Julia function than just a name. Each
method is identified by a distinct type signature. At run time, the Julia
implementation dispatches a function call to the \emph{most specific} method by
comparing the types of the arguments to the types of the parameters of all
methods of that function. For example, the call to the \c{+} function in the
example from the introduction can dispatch to one out of over two hundreds
methods for \c{+} in the standard library alone (packages can add more methods
to a function). % TODO: \figref{fig:plus}.

% TODO: figure how to make figure* work...
% \begin{figure*}
% \begin{lstlisting}[language=julia]
% # 184 methods for generic function "+":
% [1] +(x::T, y::T) where T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} in Base at int.jl:87
% [2] +(x::T, y::T) where T<:Union{Float16, Float32, Float64} in Base at float.jl:383[3] +(::Missing) in Base at missing.jl:100
% ...
% \end{lstlisting}
% \caption{Methods from the standard library}\label{plus}\label{fig:plus}
% \Description{Several of 206 methods for the \c{+} function in the Julia standard
%   library as of Julia 1.8.5}
% \end{figure*}


Julia supports a rich type language for defining method signatures. Base types
consist of either bits types (i.e. types with a direct binary representation,
like integers) or record types (called structs in Julia). Both bits types and
record types, referred to as \emph{concrete types}, can have supertypes, but all
supertypes are \emph{abstract types}. Abstract types are arranged into a
subtyping hierarchy rooted at the built-in \c{Any} type. Every value in the
program have a unique type tag that can be accessed via the \c{typeof} funciton.
%
The type language allows for further composition of these base
types using unions, tuples, and bounded existential constructors; the result of
composition can be abstract or concrete. \citet{oopsla18b} gives a detailed
discussion of the type language and of subtyping.

Any function call in a program, such as \c{res+elm} in the example above,
requires choosing one of the methods of the target function. \emph{Method
dispatch} is a multi-step process. First, the implementation obtains the
concrete types of arguments. Second, it retrieves applicable methods by checking
for subtyping between argument types and type annotations of the methods. Next,
it sorts these methods into subtype order. Finally, the call is dispatched to
the most specific method---a method such that no other applicable method is its
strict subtype. If no such method exists, an error is produced.

Function calls are pervasive in Julia, and their efficiency is crucial for
performance. However, the multi-step dispatch mechanism
make the process slow. To attain acceptable performance, the compiler attempts
to remove as many dispatch operations as it can. This optimization leverages
run-time type information whenever a method is compiled, i.e., when it is called
for the first time with a novel set of argument types.  These types are used by
the compiler to infer types in the method body. Then, this type information
frequently allows the compiler to devirtualize and inline the function calls
within a method~\cite{aigner}, thus improving performance. However, this
optimization is not always possible: if type inference cannot produce a
sufficiently specific type, then the call cannot be devirtualized.

\subsection{Type Stability: A Key To Performance?}

To illustrate a profound effect that type inference precision can have on
performance, consider the \c{sum} function from the introduction benchmarked in
three scenarios differing only in input types. Assuming a random array of integers
called \c{vint} and a random array of floating-point numbers called \c{vflt}
(both consisting of 10K elements)
compare the median running time of the following three calls to \c{sum}:
\begin{itemize}

  \item \c{sum(vint, 0)}~--- 1.397 microseconds
  \item \c{sum(vflt, 0.5)}~--- 11.489 microseconds
  \item \c{sum(vint, 0.5)}~--- 64.623 microseconds
\end{itemize}

When a performance regression occurs, it is common for Julia developers to study
the intermediate representation produced by the compiler. To facilitate this,
the language provides a utilty, \c{code_warntype}, that shows Julia's
intermediate representation of the code along with the inferred types for a given function
invocation. Types that are imprecise, i.e. abstract, show up in red: they
indicate that concrete type of a run-time value may vary from run to run.
%
In the first two benchmarks, the Julia compiler is able to deduce a concrete return
type of the method (\c{Int} and \c{Float} correspondingly), but the type of the last one
reported as \c{Union\{Int, Float\}}, which is an abstract type.
%
Such type imprecision can impact performance in two ways. First,
the \c{res} variable has to be boxed, adding a level of indirection to
any operation performed therein. Second, it is harder for
the compiler to devirtualize and inline consecutive calls, thus requiring
dynamic dispatch.

Julia's compilation model is designed to accommodate source programs
with flexible types. Yet, to make such programs efficient, the compiler
creates an \emph{instance} of each source method for each distinct tuple of
argument types. Thus, even if the programmer does not provide any type
annotations, like in the \c{sum} example, the compiler will create method
instances for \emph{concrete} input types seen during
an execution. For example, the three benchmarks shown above will make the
compiler create three distinct method instances.
Because method instances have more precise argument types, the compiler can
leverage them to produce more efficient code and infer more precise return types.

In Julia parlance, a method is called \emph{type stable} if, given a concrete
input type, it is possible to infer a concrete output type. The \c{sum} function
is not type stable because for the input type \c{Tuple\{Vector\{Int\},
Float64\}}\footnote{Tuple types encode several input arguments in Julia.}
the return type can be either \c{Int} or \c{Float64}.

\citet{Pelenitsyn21} formalize type stability as it
relates to program \emph{execution} by building a formal model of a
type-specializing just-in-time compiler that does its main job at the run time.
In this work, we set to approximate the property of type stability for
arbitrary Julia code statically, without running the code in question.

%\section{Approximating Type Stability Statically}\label{chap:approx}
\section{Inferring Type Stability Versus Inferring Types}%
\label{sec:arrive}
% \section{Inferring Types(tability)} ha-ha

A natural idea for inferring type stability in Julia would be to formulate it as
a forward static analysis: being an abstract or concrete type is one bit of
information that has a known value at the input (concrete) and should be
propagated to the output, possibly changing on the way.

To test the static analysis idea, consider a positive example first: the
identity function.
\begin{verbatim}
  function id(x)
    x
  end
\end{verbatim}
%
It is straightforward to infer that, given any concrete input type, the return
value is also concretely typed: the one bit of information carries over to the
result in one step.

Another example, the increment function, shows that the task becomes unwieldy fast.
%
\begin{verbatim}
  function inc(x)
    x + 1
  end
\end{verbatim}
%
Concreteness of the result returned by \c{inc} depends on the same property of
the result of the call to \c{+}. In turn, the property of the return type of
\c{+} depends on which \c{+} method Julia will dispatch to at the run time.
There are about two hundreds method implementations of \c{+} in the standard
library alone, and packages add more. Some of those methods are type stable
(e.g. \c{+(::Int64,::Int64)}), and some of them not (e.g.
\lstinline|+(::Rational{Bool},::Rational{Bool}|)
% \footnote{%
%   The reason for the
%   \c{+(::Rational\{Bool\},::Rational\{Bool\})} method to be not type
%   stable is not important, but in the nutshell, Julia has made a questionable
%   design decision about the return type of \c{{+}(::Bool,::Bool)}, which in the
%   current implementation is \c{Int}
%   (see discussion \url{https://github.com/JuliaLang/julia/issues/19168}),
%   and when adding two rational numbers with boolean components, depending on the
%   values of the summands, you get back either \c{Rational\{Bool\}} or
%   \c{Rational\{Int\}}.}
).
Therefore,
to infer the property of interest, in general,
we need to predict which methods are selected at the run time.

The \c{inc} example shows that inferring type stability of Julia code
requires
reasoning about which methods will be called to at the run time, which, in a
language with dynamic dispatch, leads
to reasoning about the \emph{types} of intermediate values, rather than only
the concreteness bit. But if we had a tool to compute typing information
beforehand, we would not need to build a special purpose analysis for type
stability: it suffices to ask the tool to tell the type of the return value and
check if that type is concrete. The observation of interactions between type
stability and type inference can be formulated as the following conjecture.

\begin{conjecture}
Inferring type stability of a Julia method statically is no easier than doing type
inference over that method.
\end{conjecture}

A full type inference algorithm would be enough for checking type stability of
Julia code. Should we build one from scratch? There are two reasons to do
otherwise.
\begin{enumerate}

  \item It is not clear that typing Julia as is can yield any meaningful result
  (more on this see~\cite{Chung23}).

  \item Julia already has a type inference engine built in. It was modeled
        as a black box in~\cite{Pelenitsyn21}. If we end up with a
        custom algorithm to infer types and analyze type stability based on it,
        our results may diverge from Julia's.
\end{enumerate}

\section{An Algorithm To Approximate Type Stability}%
\label{sec:algo}

If our predictions for type stability are to align with the Julia
implementation, our analysis should closely model what happens at the run time,
as described in~\cite{Pelenitsyn21}. The type-specializing JIT-compiler
from~\cite{Pelenitsyn21} makes optimization decisions based on \emph{concrete
  input types} with the help of Juila's type inference engine. Our algorithm for
approximating these decisions statically considers all (or as many as
possible) allowed concrete input types of a method ---~\figref{fig:infer-ts}.

\input{figs/infer-ts.tex}

Let us consider every step of the algorithm described
on~\figref{fig:infer-ts} and show its meaning using an example.
The list below also assigns the numbers to each step in our algorithm.
\begin{enumerate}

  \item The input of the algorithm is a Julia method. Methods in Julia are
  represented by runtime objects of type \c{Method} and can be manipulated as
  all other objects (e.g. stored in collections, responding to field accesses, etc.).

  For example, consider the \c{length} method from the Julia's standard library. We can
  get the corresponding \c{Method} object using the standard \c{@which} macro
  applied to an application of the \c{length} method. This can be done in the
  Julia REPL (signified by the \texttt{julia>} prefix).
\begin{verbatim}
  julia> @which length([1,2,3])
  length(a::Array) in Base at array.jl:215
\end{verbatim}

  The output shows that the given \c{length}-call will dispatch to the
  method defined in the \texttt{Base} module (Julia-speak for the standard
  library). The output also shows the location of the method in the
  standard library and, most importantly for us, the signature of the
  method. In fact, what we see here is a pretty-printed representation of
  the \c{Method} object representing a particular Julia method.

  \item The first task of the algorithm is to get the input type of the given
  method. This is possible through querying the \c{sig} field of the method object.

  Building on the example above, we can get the signature of the \c{length}
  method as follows:
\begin{verbatim}
  julia> m = @which length([1,2,3]);

  julia> m.sig
  Tuple{typeof(length), Array}
\end{verbatim}

  A signature of a method will usually have the special singleton function
  type (\c{typeof(...)}) as the first component, and the rest is (easy to
  convert to) the type of the input --- an $n$-tuple. In this example, the type of
  the input is 1-tuple, consisting of the existential array type
  \c{Array\{T, N\}\ where T where N} abbreviated simply as \c{Array}.

  \item The input type can be either concrete, which, in Julia, means there may
  be no proper subtypes of that type, or abstract. Still, the choice on the
  step 3 will enter the loop at least once, because if the input type is
  concrete, the check holds once trivially (e.g.\ there is exactly one concrete
  subtype of the concrete type \c{Int} --- it is \c{Int} itself).

  If the input type is abstract, we need a subprocess enumerating all concrete
  subtypes of it. We discuss an implementation for this below but it suffices to
  treat is as a black box for now. % TODO: ref to subtyping section?

  In the case of the \c{length} method, the input type, \c{Array}, is abstract
  because it is an existential type.

  \item Running type inference for a given method and a given concrete input
  type is done through calling the Julia's standard \c{code\_typed} function.
  The only issue with the function is that it expects a function object as the
  part of the input, not a method object. But getting from a method to the
  corresponding function is possible using the signature field discussed above,
  and, in particular, the singleton function type living in the first component
  of the \c{sig} field: accessing the single function object using the function
  type is possible via the \c{instance} field.

  Running type inference for the \c{length} method and the concrete input type
  \c{Array\{1, Float64\}} could be done as follows:
% \begin{minipage}{.9\textwidth}
% \begin{lstlisting}[style=jterm]
\begin{verbatim}
  julia> code_typed(m.sig.parameters[1]
                     .instance,
                    (Array{1, Float64},),
                    optimize=false)
  1-element Vector{Any}:
  CodeInfo(
  1 - %1 = Base.arraylen(a)::Int64
  +--      return %1
  ) => Int64
\end{verbatim}
% \end{lstlisting}
% \end{minipage}

  \item In the running example and for the input type \c{Array\{1, Float64\}}
  the type inferencer is able to deduce that the return type of \c{length}
  called with the given input type is \c{Int64}, which is a concrete type
  according to the standard Julia's \c{isconcretetype} predicate.
  Following the second decision element on \figref{fig:infer-ts}, we get back to
  the start of the loop and try another concrete subtype of the input type.
\end{enumerate}

\section{Implementation}%
\label{sec:impl}

The first practical consideration of the algorithm as shown is termination. It
is clear that the algorithm does not terminate for some inputs. Consider the
\c{length} example: its input type is the existential \c{Array} type, which
means that possible concrete input types may be:
\begin{itemize}

  \item \c{Array\{1, Int\}}
  \item \c{Array\{1, Array\{1, Int\}\}}
  \item \c{Array\{1, Array\{1, Array\{1, Int\}\}\}}
  \item etc.
\end{itemize}

The issue of termination is close to another one: a search space blow up.
In the standard library alone, there are over five hundreds immediate subtypes
of \c{Any} and every concrete subtype of those can be use to instantiate the
\c{Array} type when inferring stability of \c{length}. Although finite, this
space can be simply too large to exhaust in a reasonable time.

Our intention is to develop certain heuristics to perform an early termination
of the algorithm. The simplest heuristic of this sort is to employ a fuel
parameter that gives an arbitrary upper bound for the number of steps we are
allowed to perform before we give up.

Another implementation concern is step 3 where we need to generate a concrete
subtype of the input type. Although, there is no built-in facility for this
task, Julia allows getting immediate subtypes of a given type using the
\c{subtypes} method. As our current implementation shows, with enough care, it
is not hard to enumerate concrete subtypes applying the \c{subtypes} method
iteratively.

\section{Evaluation}%
\label{sec:eval}

\citet{Pelenitsyn21} analyzed type stability of a corpus of open-source Julia
packages dynamically via executing test suites of the packages and inspecting the
resulting method instances collected from the internal state of the virtual
machine. We propose to run our algorithm for statically inferring type stability
over the same packages and match the results.


\bibliography{bib/jv,bib/all,bib/lj}

\end{document}
